{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "File: FederatedLearning.ipynb\n",
        "Author: Amit Prakash\n",
        "Purpose: See how many clients/epochs affect the ML model accuracy\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAwwjuilgah0",
        "outputId": "4696331a-7a6f-4e47-e22c-6a8aab7fec5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.9/dist-packages (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.9/dist-packages (from h5py) (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (4.5.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (0.40.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install h5py\n",
        "!pip install typing-extensions\n",
        "!pip install wheel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slOdId6rzNaH",
        "outputId": "263b1c46-528e-4ab3-fece-1fdc4f598969"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.9/819.9 KB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.7/126.7 KB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m498.1/498.1 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.7/251.7 KB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.3/887.3 KB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.2/65.2 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 KB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.7/53.7 KB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.2/120.2 KB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.9/238.9 KB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 KB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.3/462.3 KB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for jax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 3.5.1 requires tqdm<5.0.0,>=4.38.0, but you have tqdm 4.28.1 which is incompatible.\n",
            "pymc 5.1.2 requires cachetools>=4.2.1, but you have cachetools 3.1.1 which is incompatible.\n",
            "prophet 1.1.2 requires tqdm>=4.36.1, but you have tqdm 4.28.1 which is incompatible.\n",
            "panel 0.14.4 requires tqdm>=4.48.0, but you have tqdm 4.28.1 which is incompatible.\n",
            "pandas-profiling 3.2.0 requires tqdm>=4.48.2, but you have tqdm 4.28.1 which is incompatible.\n",
            "orbax 0.1.6 requires jax>=0.4.6, but you have jax 0.2.28 which is incompatible.\n",
            "grpcio-status 1.48.2 requires grpcio>=1.48.2, but you have grpcio 1.34.1 which is incompatible.\n",
            "google-cloud-bigquery 3.4.2 requires grpcio<2.0dev,>=1.47.0, but you have grpcio 1.34.1 which is incompatible.\n",
            "flax 0.6.7 requires jax>=0.4.2, but you have jax 0.2.28 which is incompatible.\n",
            "chex 0.1.7 requires jax>=0.4.6, but you have jax 0.2.28 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install --quiet --upgrade tensorflow-federated==0.20.0\n",
        "!pip install --quiet --upgrade nest-asyncio\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufBmYoGUIvwr",
        "outputId": "0b589f68-39f9-4a3f-819a-34a1716c1d72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.8.0\n",
            "  Downloading tensorflow-2.8.0-cp39-cp39-manylinux2010_x86_64.whl (497.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.6/497.6 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow_probability==0.14.1\n",
            "  Downloading tensorflow_probability-0.14.1-py2.py3-none-any.whl (5.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.0) (67.6.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.0) (2.0.7)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.0) (0.31.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.0) (1.15.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.0) (2.8.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.0) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.0) (1.1.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.0) (3.8.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.0) (1.16.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.0) (3.3.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.0) (16.0.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.0) (2.8.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.0) (3.19.6)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.0) (1.21.6)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 KB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.0) (1.0.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.0) (1.34.1)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.0) (0.4.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.0) (2.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.0) (4.5.0)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow_probability==0.14.1) (2.2.1)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.9/dist-packages (from tensorflow_probability==0.14.1) (0.1.8)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from tensorflow_probability==0.14.1) (4.4.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow==2.8.0) (0.40.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.16.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.27.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.2.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (6.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.26.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.2.2)\n",
            "Installing collected packages: tf-estimator-nightly, tensorflow_probability, tensorflow\n",
            "  Attempting uninstall: tensorflow_probability\n",
            "    Found existing installation: tensorflow-probability 0.19.0\n",
            "    Uninstalling tensorflow-probability-0.19.0:\n",
            "      Successfully uninstalled tensorflow-probability-0.19.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.4\n",
            "    Uninstalling tensorflow-2.8.4:\n",
            "      Successfully uninstalled tensorflow-2.8.4\n",
            "Successfully installed tensorflow-2.8.0 tensorflow_probability-0.14.1 tf-estimator-nightly-2.8.0.dev2021122109\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow==2.8.0 tensorflow_probability==0.14.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5PmJBlhzzt6"
      },
      "outputs": [],
      "source": [
        "# Import all necessary libraries/modules\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "import collections\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8mPsnL5zS1H"
      },
      "outputs": [],
      "source": [
        "# Set seed\n",
        "SEED = 200372055 \n",
        "np.random.seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m03_8F9izTT2"
      },
      "outputs": [],
      "source": [
        "# Preprocess the input data \n",
        "def preprocess(dataset, epoch):\n",
        "  def batch_format_fn(element):\n",
        "    \"\"\"Flatten a batch `pixels` and return the features as an `OrderedDict`.\"\"\"\n",
        "    return collections.OrderedDict(\n",
        "        x=tf.reshape(element['pixels'], [-1, 784]),\n",
        "        y=tf.reshape(element['label'], [-1, 1]))\n",
        "\n",
        "  return dataset.repeat(epoch).shuffle(100, seed=SEED).batch(\n",
        "      20).map(batch_format_fn).prefetch(10)\n",
        "\n",
        "# Combine data from multiple clients\n",
        "def make_federated_data(client_data, client_ids, epoch):\n",
        "  return [\n",
        "      preprocess(client_data.create_tf_dataset_for_client(x), epoch)\n",
        "      for x in client_ids\n",
        "  ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQBx7Z70zXYw",
        "outputId": "cf21c8b9-d0b7-484d-e672-b33388ecb414"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading emnist_all.sqlite.lzma: 100%|██████████| 170507172/170507172 [00:44<00:00, 3945280.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of clients:  3383\n"
          ]
        }
      ],
      "source": [
        "# Download the MNIST data \n",
        "emnist_train, emnist_test = tff.simulation.datasets.emnist.load_data()\n",
        "print (\"Total number of clients: \",len(emnist_train.client_ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbM9xB28zck_"
      },
      "outputs": [],
      "source": [
        "# Determine the sample data input data structure for ML model \n",
        "example_dataset = emnist_train.create_tf_dataset_for_client(emnist_train.client_ids[0])\n",
        "preprocessed_example_dataset = preprocess(example_dataset, 0)\n",
        "\n",
        "# Neural network keras model\n",
        "def create_keras_model():\n",
        "  return tf.keras.models.Sequential([\n",
        "      tf.keras.layers.InputLayer(input_shape=(784,)),\n",
        "      tf.keras.layers.Dense(10, kernel_initializer='zeros'),\n",
        "      tf.keras.layers.Softmax(),\n",
        "  ])\n",
        "  \n",
        "def model_fn():\n",
        "  # We must create a new model here, and not capture it from an external\n",
        "  # scope. TFF will call this within different graph contexts.\n",
        "  keras_model = create_keras_model()\n",
        "  return tff.learning.from_keras_model(\n",
        "      keras_model,\n",
        "      input_spec=preprocessed_example_dataset.element_spec,\n",
        "      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkcJGifXzhVZ",
        "outputId": "6f41d7d6-190b-43c9-8015-9be6f826752c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Client IDs selected:  ['f0969_31' 'f3590_03' 'f1089_04' 'f1432_44' 'f1765_33']\n",
            "Number of client datasets considered: 5\n",
            "round  1, training accuracy= 13.188010454177856%\n",
            "round  2, training accuracy= 15.422342717647552%\n",
            "round  3, training accuracy= 19.073569774627686%\n",
            "round  4, training accuracy= 22.45231568813324%\n",
            "round  5, training accuracy= 21.58038169145584%\n",
            "round  6, training accuracy= 29.536783695220947%\n",
            "round  7, training accuracy= 34.71389710903168%\n",
            "round  8, training accuracy= 39.29155170917511%\n",
            "round  9, training accuracy= 40.21798372268677%\n",
            "round 10, training accuracy= 47.247955203056335%\n",
            "Test Accuracy: 59.090906381607056%\n",
            "Client IDs selected:  ['f0458_38' 'f0305_08' 'f3447_20' 'f1113_16' 'f0744_23']\n",
            "Number of client datasets considered: 5\n",
            "round  1, training accuracy= 31.67780041694641%\n",
            "round  2, training accuracy= 58.117878437042236%\n",
            "round  3, training accuracy= 74.79764223098755%\n",
            "round  4, training accuracy= 82.14145302772522%\n",
            "round  5, training accuracy= 85.65815091133118%\n",
            "round  6, training accuracy= 87.84282803535461%\n",
            "round  7, training accuracy= 89.25343751907349%\n",
            "round  8, training accuracy= 90.40864706039429%\n",
            "round  9, training accuracy= 91.23772382736206%\n",
            "round 10, training accuracy= 91.76031351089478%\n",
            "Test Accuracy: 77.4193525314331%\n",
            "Client IDs selected:  ['f3767_45' 'f4097_41' 'f2286_50' 'f3922_38' 'f1673_32']\n",
            "Number of client datasets considered: 5\n",
            "round  1, training accuracy= 59.0597927570343%\n",
            "round  2, training accuracy= 84.5835030078888%\n",
            "round  3, training accuracy= 92.68866181373596%\n",
            "round  4, training accuracy= 94.95051503181458%\n",
            "round  5, training accuracy= 95.8164930343628%\n",
            "round  6, training accuracy= 96.32577300071716%\n",
            "round  7, training accuracy= 96.70721888542175%\n",
            "round  8, training accuracy= 96.91340327262878%\n",
            "round  9, training accuracy= 97.11958765983582%\n",
            "round 10, training accuracy= 97.34227061271667%\n",
            "Test Accuracy: 87.93103694915771%\n"
          ]
        }
      ],
      "source": [
        "NUM_EPOCHS = 5 # Change the number of training epoch for local training by each client in next block\n",
        "\n",
        "## Iteratively change NUM_CLIENTS\n",
        "for NUM_CLIENTS in [5, 50, 100]:\n",
        "  sample_clients = np.random.choice(emnist_train.client_ids, NUM_CLIENTS)\n",
        "  print (\"Client IDs selected: \", sample_clients)\n",
        "\n",
        "  # Consider data from only the selected clients\n",
        "  federated_train_data = make_federated_data(emnist_train, sample_clients, NUM_EPOCHS)\n",
        "  print(f'Number of client datasets considered: {len(sample_clients)}')\n",
        "\n",
        "  # Initialize the iterative training object with the right learning parameter\n",
        "  iterative_process = tff.learning.build_federated_averaging_process(\n",
        "      model_fn,\n",
        "      client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.01),\n",
        "      server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0))\n",
        "\n",
        "  # Initialize the parameters of the ML model (you need to initialize this each time you change the client number or epoch number)\n",
        "  state = iterative_process.initialize()\n",
        "\n",
        "  # Total number of server and client interactions\n",
        "  NUM_ROUNDS = 11\n",
        "  for round_num in range(1, NUM_ROUNDS):\n",
        "    state, metrics = iterative_process.next(state, federated_train_data)\n",
        "    print('round {:2d}, training accuracy= {}%'.format(round_num, metrics['train']['sparse_categorical_accuracy']*100))\n",
        "\n",
        "  # Evaluate the latest converged model \n",
        "  evaluation = tff.learning.build_federated_evaluation(model_fn)\n",
        "  federated_test_data = make_federated_data(emnist_test, sample_clients, 5)\n",
        "  test_metrics = evaluation(state.model, federated_test_data)\n",
        "  print('Test Accuracy: {}%'.format(str(test_metrics['eval']['sparse_categorical_accuracy']*100)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQzTkKi-uNzO",
        "outputId": "6f41d7d6-190b-43c9-8015-9be6f826752c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Client IDs selected:  ['f0969_31' 'f3590_03' 'f1089_04' 'f1432_44' 'f1765_33']\n",
            "Number of client datasets considered: 5\n",
            "round  1, training accuracy= 13.188010454177856%\n",
            "round  2, training accuracy= 15.422342717647552%\n",
            "round  3, training accuracy= 19.073569774627686%\n",
            "round  4, training accuracy= 22.45231568813324%\n",
            "round  5, training accuracy= 21.58038169145584%\n",
            "round  6, training accuracy= 29.536783695220947%\n",
            "round  7, training accuracy= 34.71389710903168%\n",
            "round  8, training accuracy= 39.29155170917511%\n",
            "round  9, training accuracy= 40.21798372268677%\n",
            "round 10, training accuracy= 47.247955203056335%\n",
            "Test Accuracy: 59.090906381607056%\n",
            "Client IDs selected:  ['f0458_38' 'f0305_08' 'f3447_20' 'f1113_16' 'f0744_23']\n",
            "Number of client datasets considered: 5\n",
            "round  1, training accuracy= 31.67780041694641%\n",
            "round  2, training accuracy= 58.117878437042236%\n",
            "round  3, training accuracy= 74.79764223098755%\n",
            "round  4, training accuracy= 82.14145302772522%\n",
            "round  5, training accuracy= 85.65815091133118%\n",
            "round  6, training accuracy= 87.84282803535461%\n",
            "round  7, training accuracy= 89.25343751907349%\n",
            "round  8, training accuracy= 90.40864706039429%\n",
            "round  9, training accuracy= 91.23772382736206%\n",
            "round 10, training accuracy= 91.76031351089478%\n",
            "Test Accuracy: 77.4193525314331%\n",
            "Client IDs selected:  ['f3767_45' 'f4097_41' 'f2286_50' 'f3922_38' 'f1673_32']\n",
            "Number of client datasets considered: 5\n",
            "round  1, training accuracy= 59.0597927570343%\n",
            "round  2, training accuracy= 84.5835030078888%\n",
            "round  3, training accuracy= 92.68866181373596%\n",
            "round  4, training accuracy= 94.95051503181458%\n",
            "round  5, training accuracy= 95.8164930343628%\n",
            "round  6, training accuracy= 96.32577300071716%\n",
            "round  7, training accuracy= 96.70721888542175%\n",
            "round  8, training accuracy= 96.91340327262878%\n",
            "round  9, training accuracy= 97.11958765983582%\n",
            "round 10, training accuracy= 97.34227061271667%\n",
            "Test Accuracy: 87.93103694915771%\n"
          ]
        }
      ],
      "source": [
        "NUM_CLIENTS = 5 # Change number of clients as needed in previous block\n",
        "\n",
        "## Iteratively change NUM_EPOCHS\n",
        "for NUM_EPOCHS in [5, 50, 100]:\n",
        "  sample_clients = np.random.choice(emnist_train.client_ids, NUM_CLIENTS)\n",
        "  print (\"Client IDs selected: \", sample_clients)\n",
        "\n",
        "  # Consider data from only the selected clients\n",
        "  federated_train_data = make_federated_data(emnist_train, sample_clients, NUM_EPOCHS)\n",
        "  print(f'Number of client datasets considered: {len(sample_clients)}')\n",
        "\n",
        "  # Initialize the iterative training object with the right learning parameter\n",
        "  iterative_process = tff.learning.build_federated_averaging_process(\n",
        "      model_fn,\n",
        "      client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.01),\n",
        "      server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0))\n",
        "\n",
        "  # Initialize the parameters of the ML model (you need to initialize this each time you change the client number or epoch number)\n",
        "  state = iterative_process.initialize()\n",
        "\n",
        "  # Total number of server and client interactions\n",
        "  NUM_ROUNDS = 11\n",
        "  for round_num in range(1, NUM_ROUNDS):\n",
        "    state, metrics = iterative_process.next(state, federated_train_data)\n",
        "    print('round {:2d}, training accuracy= {}%'.format(round_num, metrics['train']['sparse_categorical_accuracy']*100))\n",
        "\n",
        "  # Evaluate the latest converged model \n",
        "  evaluation = tff.learning.build_federated_evaluation(model_fn)\n",
        "  federated_test_data = make_federated_data(emnist_test, sample_clients, 5)\n",
        "  test_metrics = evaluation(state.model, federated_test_data)\n",
        "  print('Test Accuracy: {}%'.format(str(test_metrics['eval']['sparse_categorical_accuracy']*100)))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
